# -*- coding: utf-8 -*-
"""Language Transformer 7.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NRcleD9n88nqmUNPlAK7IrLUa2WOx24L

# **0. Imports**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""# **1. Data**"""

sheet_url = "https://docs.google.com/spreadsheets/d/1zFxstQlrdaKkV3fDIPNXTwdijy2CjLfZOH0EU07dDQk/edit#gid=0"
url_1 = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')

df = pd.read_csv(url_1)
startToken = [['<start>']]
words = list(df)
df_vocab = df[words].astype(str)
Vocab = df_vocab.to_numpy()

"""# **2. Hyperparameters**"""

dm = 512         # Model dimension
h = 8            # Number of heads
dk = int(dm / h) # Dimension of Q and K
dv = dk          # Dimension of V
N = 3            # Number of Encoder/Decoder Stacks
v = len(Vocab)   # Number of words in vocabulary
r = 2048         # Number of neurons in Feed Forward hidden layer
epsilon = 0.0001

"""# **3. Functions**

3.1 Activations
"""

def softmax(X):
  maxVal = np.max(X)
  X -= maxVal
  X = X.astype(np.longdouble)
  return np.exp(X)/np.sum(np.exp(X))

def ReLU(x):
  return np.maximum(0, x)

def norm(X):
  m, n = np.shape(X)
  temp = np.zeros((m, n))
  i = 0
  while i < m:
    row = X[i,:]
    mu = np.average(row)
    var = np.var(row)
    j = 0
    while j < n:
      temp[i, j] = (X[i,j] - mu) / np.sqrt(var + epsilon)
      j += 1
    i += 1
  return temp

"""3.2 Word Processing"""

def getVocabIdx(word):
  # Takes in str of word, return int of vocab index
  return df.index[df['Word'] == word].to_list()[0]

def S2Vec(x):
  # Takes in str of sentence, returns vec (1, words+2) of word indices
  S = x.split()
  words = len(S)
  SVec = np.zeros((1,words+2), dtype=int)
  i = 0
  while i < words:
    word = S[i]
    SVec[0,i+1] = int(getVocabIdx(word))
    i += 1
  SVec[0,0] = 0
  SVec[0,words+1] = 1
  return SVec

def get_I(X):
  # Takes in sentence vec (1, words) & returns input embedding mat (dm, words)
  words = np.shape(X)[1]
  I = np.zeros((dm, words))
  i = 0
  while i < words:
    wordIdx = X[0,i]
    I[:,i] = I_db[:, wordIdx]
    i += 1
  return I

def get_P(X):
  # Takes in sentence vec (1, words) & returns position embedding mat (dm, words)
  words = np.shape(X)[1]
  P = np.zeros((dm, words))
  j = 0
  while j < words:
    i = 0
    while i < dm:
      if i % 2 == False:
        P[i,j] = np.sin(j/(10000**(2*i/dm)))
        i += 1
      else:
        P[i,j] = np.cos(j/(10000**(2*i/dm)))
        i += 1
    j += 1
  return P

def get_Mask(w):
  temp = np.ones((w,w))
  i = 0
  while i < w:
    j = 0
    while j < w:
      if i < j:
        temp[i,j] = 0
        j += 1
      else:
        j += 1
    i += 1
  return temp

"""3.3 Initialization"""

def init_params(dm, h, dk, dv, N, v, r):
  global I_db, E_W_Q, E_W_K, E_W_V, E_W_O, E_W_FF1, E_W_FF2, D1_W_Q, D1_W_K
  global D1_W_V, D1_W_O, D2_W_Q, D2_W_K, D2_W_V, D2_W_O, D2_W_FF1, D2_W_FF2
  global D2_W_L, E_b_FF1, E_b_FF2, D2_b_FF1, D2_b_FF2
  # Embeddings
  I_db = np.random.rand(dm, v)
  # Encoder
  E_W_Q = np.random.rand(dm, dk, h)
  E_W_K = np.random.rand(dm, dk, h)
  E_W_V = np.random.rand(dm, dv, h)
  E_W_O = np.random.rand(dm, dm)
  E_W_FF1 = np.random.rand(dm, r)
  E_b_FF1 = np.random.rand(1, r)
  E_W_FF2 = np.random.rand(r, dm)
  E_b_FF2 = np.random.rand(1, dm)
  # Decoder
  D1_W_Q = np.random.rand(dm, dk, h)
  D1_W_K = np.random.rand(dm, dk, h)
  D1_W_V = np.random.rand(dm, dv, h)
  D1_W_O = np.random.rand(dm, dm)
  D2_W_Q = np.random.rand(dm, dk, h)
  D2_W_K = np.random.rand(dm, dk, h)
  D2_W_V = np.random.rand(dm, dv, h)
  D2_W_O = np.random.rand(dm, dm)
  D2_W_FF1 = np.random.rand(dm, r)
  D2_b_FF1 = np.random.rand(1, r)
  D2_W_FF2 = np.random.rand(r, dm)
  D2_b_FF2 = np.random.rand(1, dm)
  D2_W_L = np.random.rand(dm, v)

"""3.4 Forward Propagation"""

def encoder(X):
  global w, E_I, E_P, E_E, E_Q, E_K, E_V, E_A, E_M, E_M_0, E_F, O_E
  # Input Pre-Processing
  w = np.shape(X)[1]
  E_I = get_I(X)
  E_P = get_P(X)
  E_E = E_I + E_P
  # Initialize E_Q, E_K, E_V, E_A_h
  E_Q = np.zeros((w, dk, h))
  E_K = np.zeros((w, dk, h))
  E_V = np.zeros((w, dv, h))
  E_A_h = np.zeros((w, dv, h))
  # Multi-Head Attention
  head = 0
  while head < h:
    E_Q[:,:,head] = np.dot(E_E.T, E_W_Q[:,:,head])
    E_K[:,:,head] = np.dot(E_E.T, E_W_K[:,:,head])
    E_V[:,:,head] = np.dot(E_E.T, E_W_V[:,:,head])
    E_A_h[:,:,head] = np.dot(softmax((1/np.sqrt(dk))*np.dot(E_Q[:,:,head], E_K[:,:,head].T)), E_V[:,:,head])
    head += 1
  # Concatenate E_A_h and broadcast using E_W_O
  head = 1
  E_A = E_A_h[:,:,0]
  while head < h:
    E_A = np.concatenate((E_A, E_A_h[:,:,head]),axis=1)
    head += 1
  E_M = np.dot(E_A, E_W_O)
  # Add residual connection and Norm
  E_M_0 = norm(E_E.T + E_M)
  # Feed Forward and Norm
  E_F = norm(np.dot(ReLU(np.dot(E_M_0, E_W_FF1) + E_b_FF1), E_W_FF2) + E_b_FF2)
  # Add residual connection and Norm
  O_E = norm(E_M_0 + E_F)
  return O_E

def decoder(genOut, O_E):
  global w_0, D_I, D_P, D_E
  global Mask, D1_Q, D1_K, D1_V, D1_A, D1_M
  global D2_Q, D2_K, D2_V, D2_A, D2_M, D2_M_0, D2_F, O_D, Z_0, Yhat
  # Input Pre-Processing
  w_0 = np.shape(genOut)[1]
  D_I = get_I(genOut)
  D_P = get_P(genOut)
  D_E = D_I + D_P
  # Initialize D1_Q, D1_K, D1_V, D1_A_h and get Mask
  Mask = get_Mask(w_0)
  D1_Q = np.zeros((w_0, dk, h))
  D1_K = np.zeros((w_0, dk, h))
  D1_V = np.zeros((w_0, dv, h))
  D1_A_h = np.zeros((w_0, dv, h))
  # Masked Multi-Head Attention
  head = 0
  while head < h:
    D1_Q[:,:,head] = np.dot(D_E.T, D1_W_Q[:,:,head])
    D1_K[:,:,head] = np.dot(D_E.T, D1_W_K[:,:,head])
    D1_V[:,:,head] = np.dot(D_E.T, D1_W_V[:,:,head])
    D1_A_h[:,:,head] = np.dot(softmax(np.multiply(Mask, (1/np.sqrt(dk))*np.dot(D1_Q[:,:,head], D1_K[:,:,head].T))), D1_V[:,:,head])
    head += 1
  # Concatenate D1 A_h and broadcast using D1_W_O
  head = 1
  D1_A = D1_A_h[:,:,0]
  while head < h:
    D1_A = np.concatenate((D1_A, D1_A_h[:,:,head]),axis=1)
    head += 1
  D1_M = norm(D_E.T + np.dot(D1_A, D1_W_O))
  # Initialize D2_Q, D2_K, D2_V, D2_A_h
  D2_Q = np.zeros((w_0, dk, h))
  D2_K = np.zeros((w, dk, h))
  D2_V = np.zeros((w, dv, h))
  D2_A_h = np.zeros((w_0, dv, h))
  # Multi-Head Attention
  head = 0
  while head < h:
    D2_Q[:,:,head] = np.dot(D1_M, D2_W_Q[:,:,head])
    D2_K[:,:,head] = np.dot(O_E, D2_W_K[:,:,head])
    D2_V[:,:,head] = np.dot(O_E, D2_W_V[:,:,head])
    D2_A_h[:,:,head] = np.dot(softmax((1/np.sqrt(dk))*np.dot(D2_Q[:,:,head], D2_K[:,:,head].T)), D2_V[:,:,head])
    head += 1
  # Concatenate D2_A_h and broadcast using D2_W_O
  head = 1
  D2_A = D2_A_h[:,:,0]
  while head < h:
    D2_A = np.concatenate((D2_A, D2_A_h[:,:,head]),axis=1)
    head += 1
  D2_M = np.dot(D2_A, D2_W_O)
  # Add and Norm
  D2_M_0 = norm(D1_M + D2_M)
  # Feed Forward and Norm
  D2_F = norm(np.dot(ReLU(np.dot(D2_M_0, D2_W_FF1) + D2_b_FF1), D2_W_FF2) + D2_b_FF2)
  O_D = norm(D2_M_0 + D2_F)
  # Logits (Z) and concat to Z_0
  Z = np.dot(O_D, D2_W_L)
  Z_0 = Z[0,:]
  k = 1
  while k < w_0:
    Z_0 += Z[k,:]
    k += 1
  # Softmax Z_0 for Yhat and return Yhat
  Yhat = np.reshape(softmax(Z_0), (v,1))
  return Yhat

def one_hot_Y(wordIdx):
  Y = np.zeros((v,1))
  Y[wordIdx,0] = 1
  return Y

"""3.5 Backwards Propagation"""

def norm_prime(X):
  m, n = np.shape(X)
  temp = np.zeros((m,n))
  row = 0
  while row < m:
    rowMu = np.average(X[row,:])
    rowVar = np.var(X[row,:])
    col = 0
    while col < n:
      temp[row,col] = -1*(X[row,col]-rowMu)*epsilon/(2*np.sqrt(rowVar+epsilon)*(rowVar+epsilon))
      col += 1
    row += 1
  return temp

def ReLU_prime(x):
  m, n = np.shape(x)
  temp = np.zeros((m,n))
  row = 0
  while row<m:
    col = 0
    while col<n:
      if x[row,col] > 0:
        temp[row,col] = 1
        col+=1
      else:
        col+=1
    row += 1
  return temp

def softmax_prime(X):
  rows, cols = np.shape(X)
  temp = np.zeros((rows, cols))
  Xmax = np.max(X)
  X -= Xmax
  Xsum = np.sum(np.exp(X))
  row = 0
  while row < rows:
    col = 0
    while col < cols:
      e_xij = np.exp(X[row, col])
      c = Xsum - e_xij
      temp[row, col] = c*e_xij / (Xsum**2)
      col += 1
    row += 1
  return temp


def get_gradients():
  global dZ, dOD, dF, dC, B, temp2, d_temp, d_M_0, d_D2_M_0, d_D2_M, qi, ki, vi, d_D2_Ai, d_temp, d_temp2, d_temp3, d_D1_A, d_D2_A, d_D1_Ai, d_D1_vi, d_temp7, d_temp8, d_D_E, d_D2_b_FF2, temp
  ### DECODER ###
  d_Z = np.dot((Yhat-Y), np.ones((1, w_0))).T     # (w_0, v)
  d_O_D = np.dot(d_Z, D2_W_L.T)     # (w_0, dm)
  # D2_W_L
  d_D2_W_L = np.dot(d_Z.T, O_D).T     # (dm, v)
  # FF W&b - both layers
  d_F = np.multiply(norm_prime(D2_M_0 + D2_F), d_O_D)     # (w_0, dm)
  inner = np.dot(D2_M_0, D2_W_FF1) + D2_b_FF1     # (w_0, r)
  outer = np.dot(ReLU(inner), D2_W_FF2) + D2_b_FF2     # (w_0, dm)
  temp100 = np.multiply(d_F, norm_prime(outer))     # (w_0, dm)
  d_D2_W_FF2 = np.dot(temp100.T, ReLU(inner)).T     # (r, dm)
  d_D2_b_FF2 = np.dot(np.ones((1,w_0)), temp100)     # (1, dm)
  temp101 = np.multiply(np.dot(temp100, D2_W_FF2.T), ReLU_prime(inner))     # (w_0, r)
  d_D2_W_FF1 = np.dot(temp101.T, D2_M_0).T     # (dm, r)
  d_D2_b_FF1 = np.dot(np.ones((1,w_0)), temp101)     # (1, r)
  # d_D2_W_O
  d_D2_M_0 = np.multiply(norm_prime(D2_M_0 + D2_F), d_O_D)     # (w_0, dm)
  d_D2_M = np.multiply(d_D2_M_0, norm_prime(D1_M + D2_M))     # (w_0, dm)
  d_D2_W_O = np.dot(d_D2_M.T, D2_A)     # (h*dv, dm)
  d_D1_M = np.multiply(norm_prime(D2_M_0 + D2_F), d_O_D)     # (w_0, dm) ## come back to !!!!!!!!!!!!!!!!
  d_D2_A = np.dot(d_D2_M, D2_W_O.T)     # (w_0, h*dv)
  d_D2_W_Q = np.zeros((dm, dk, h))
  d_D2_W_K = np.zeros((dm, dk, h))
  d_D2_W_V = np.zeros((dm, dv, h))
  d_O_E = np.zeros((w, dm))
  head = 0
  while head < h:
    qi = D2_Q[:,:,head]
    ki = D2_K[:,:,head]
    vi = D2_V[:,:,head]
    d_D2_Ai = d_D2_A[:,head*dv:(head+1)*dv]     # (w_0, dv)
    d_temp = np.dot(d_D2_Ai, vi.T)     # (w_0, w)
    d_temp2 = np.multiply(d_temp, softmax_prime((1/(dk**0.5))*np.dot(qi, ki.T)))     # (w_0, w)
    d_qi = np.dot(d_temp2, (1/(dk**0.5))*ki)     # (w_0, dk)
    # D2_W_Q
    d_D2_W_Q[:,:,head] = np.dot(D1_M.T, d_qi)     # (dm, dk)
    d_ki = np.dot(d_temp2.T, (1/(dk**0.5))*qi)     # (w, dk)
    # D2_W_K
    d_D2_W_K[:,:,head] = np.dot(O_E.T, d_ki)     # (dm, dk)
    d_O_Ei = np.dot(d_ki, D2_W_K[:,:,head].T)
    d_O_E += d_O_Ei
    d_vi = np.dot(softmax((1/np.sqrt(dk))*np.dot(qi,ki.T)).T, d_D2_Ai)     # (w, dv)
    # D2_W_V
    d_D2_W_V[:,:,head] = np.dot(O_E.T, d_vi)     # (dm, dv)
    d_O_Ei = np.dot(d_vi, D2_W_V[:,:,head].T)
    d_O_E += d_O_Ei
    head += 1
  d_D1_M = np.multiply(d_D2_M_0, norm_prime(D1_M + D2_M))     # (w_0, dm)
  d_temp6 = np.multiply(norm_prime(D_E.T + np.dot(D1_A, D1_W_O)), d_D1_M)     # (w_0, dm)
  d_D_E = d_temp6.T     # (dm, w_0)
  #D1_W_O
  d_D1_W_O = np.dot(d_temp6.T, D1_A).T     # (h*dv, dm)
  d_D1_A = np.dot(d_temp6, D1_W_O.T)     # (w, h*dv)
  d_D1_W_Q = np.zeros((dm, dk, h))
  d_D1_W_K = np.zeros((dm, dk, h))
  d_D1_W_V = np.zeros((dm, dv, h))
  head = 0
  while head < h:
    qi = D1_Q[:,:,head]
    ki = D1_K[:,:,head]
    vi = D1_V[:,:,head]
    d_D1_Ai = d_D1_A[:,head*dv:(head+1)*dv]     # (w_0, dv)
    d_D1_vi = np.dot(d_D1_Ai.T, softmax(np.multiply(Mask, (1/np.sqrt(dk))*np.dot(qi, ki.T)))).T     # (w_0, dv)
    # D1_W_V
    d_D1_W_V[:,:,head] = np.dot(D_E, d_D1_vi)     # (dm, dv)
    d_temp7 = np.dot(d_D1_Ai, vi.T)     # (w_0, w_0)
    d_Pi_M = np.multiply(d_temp7, softmax_prime(np.multiply(Mask, (1/np.sqrt(dk))*np.dot(qi, ki.T))))
    d_Pi = np.multiply(d_Pi_M, Mask)     # (w_0, w_0)
    d_qi = (1/np.sqrt(dk)) * np.dot(d_Pi, ki)     # (w_0, dk)
    #D1_W_Q
    d_D1_W_Q[:,:,head] = np.dot(D_E, d_qi)     # (dm, dk)
    d_ki = (1/np.sqrt(dk)) * np.dot(d_Pi, qi)     # (w_0, dk)
    # D1_W_K
    d_D1_W_K[:,:,head] = np.dot(D_E, d_ki)     # (dm, dk)
    head += 1
  d_I_db = np.zeros((dm, v))
  z = 0
  while z < w_0:
    idx = genOut[0,z]
    d_I_db[:, idx] += d_D_E[:, z]
    z += 1
  ### ENCODER ###
  # FF W&b - both layers
  d_E_F = np.multiply(d_O_E, norm_prime(E_M_0 + E_F))     # (w, dm)
  inner = np.dot(E_M_0, E_W_FF1) + E_b_FF1     # (w, r)
  outer = np.dot(ReLU(inner), E_W_FF2) + E_b_FF2     # (w, dm)
  d_temp110 = np.multiply(d_E_F, norm_prime(outer))     # (w, dm)
  d_E_W_FF2 = np.dot(ReLU(inner).T, d_temp110)     # (r, dm)
  d_E_b_FF2 = np.dot(np.ones((1,w)), d_E_F)     # (1, dm)
  d_temp111 = np.multiply(np.dot(d_temp110, E_W_FF2.T), ReLU_prime(inner))     # (w, r)
  d_E_W_FF1 = np.dot(E_M_0.T, d_temp111)     # (dm, r)
  d_E_b_FF1 = np.dot(np.ones((1,w)), d_temp111)     # (1, r)
  d_E_M_0 = np.multiply(d_O_E, norm_prime(E_M_0 + E_F))     # (w, dm)
  d_E_M = np.multiply(d_E_M_0, norm_prime(E_E.T + E_M))     # (w, dm)
  # E_W_O
  d_E_W_O = np.dot(E_A.T, d_E_M)     # (h*dv, dm)
  d_E_A = np.dot(d_E_M, E_W_O.T)     # (w, h*dv)
  d_E_W_Q = np.zeros((dm, dk, h))
  d_E_W_K = np.zeros((dm, dk, h))
  d_E_W_V = np.zeros((dm, dv, h))
  head = 0
  while head < h:
    qi = E_Q[:,:,head]
    ki = E_K[:,:,head]
    vi = E_V[:,:,head]
    d_E_Ai = d_E_A[:,head*dv:(head+1)*dv]     # (w, dv)
    # E_W_V
    d_vi = np.dot(d_E_Ai.T, softmax(1/(dk**0.5)*np.dot(qi, ki.T))).T     # (w, dv)
    d_E_W_V[:,:,head] = np.dot(E_E, d_vi)     # (dm, dv)
    # E_W_Q
    d_temp300 = np.dot(d_E_Ai, vi.T)     # (w, w)
    d_temp301 = np.multiply(softmax_prime(1/(dk**0.5)*np.dot(qi, ki.T)), d_temp300)     # (w, w)
    # E_W_Q
    d_qi = np.dot(d_temp301, 1/(dk**0.5)*ki)     # (w, dk)
    d_E_W_Q[:,:,head] = np.dot(E_E, d_qi)     # (dm, dk)
    # E_W_K
    d_ki = np.dot(d_temp301.T, 1/(dk**0.5)*qi)     # (w, dk)
    d_E_W_K[:,:,head] = np.dot(E_E, d_ki)     # (dm, dk)
    head += 1
  d_E_E = np.multiply(d_E_M_0, norm_prime(E_E.T + E_M)).T
  z = 0
  while z < w:
    idx = trainX[0,z]
    d_I_db[:, idx] += d_E_E[:, z]
    z += 1
  return d_D2_W_L, d_D2_W_FF2, d_D2_b_FF2, d_D2_W_FF1, d_D2_b_FF1, d_D2_W_O, d_D2_W_Q, d_D2_W_K, d_D2_W_V, d_I_db, d_D1_W_O, d_D1_W_V, d_D1_W_Q, d_D1_W_K, d_E_W_FF2, d_E_b_FF2, d_E_W_FF1, d_E_b_FF1, d_E_W_O, d_E_W_V, d_E_W_Q, d_E_W_K

def update_params():
  global I_db, E_W_Q, E_W_K, E_W_V, E_W_O, E_W_FF1, E_W_FF2, D1_W_Q, D1_W_K
  global D1_W_V, D1_W_O, D2_W_Q, D2_W_K, D2_W_V, D2_W_O, D2_W_FF1, D2_W_FF2
  global D2_W_L, E_b_FF1, E_b_FF2, D2_b_FF1, D2_b_FF2
  D2_W_L -= lrate*d_D2_W_L
  D2_W_FF2 -= lrate*d_D2_W_FF2
  D2_b_FF2 -= lrate*d_D2_b_FF2
  D2_W_FF1 -= lrate*d_D2_W_FF1
  D2_b_FF1 -= lrate*d_D2_b_FF1
  D2_W_O -= lrate*d_D2_W_O
  D2_W_Q -= lrate*d_D2_W_Q
  D2_W_K -= lrate*d_D2_W_K
  D2_W_V -= lrate*d_D2_W_V
  D1_W_O -= lrate*d_D1_W_O
  D1_W_Q -= lrate*d_D1_W_Q
  D1_W_K -= lrate*d_D1_W_K
  D1_W_V -= lrate*d_D1_W_V
  I_db -= lrate*d_I_db
  E_W_FF2 -= lrate*d_E_W_FF2
  E_W_FF1 -= lrate*d_E_W_FF1
  E_W_O -= lrate*d_E_W_O
  E_W_Q -= lrate*d_E_W_Q
  E_W_K -= lrate*d_E_W_K
  E_W_V -= lrate*d_E_W_V
  E_b_FF2 -= lrate*d_E_b_FF2
  E_b_FF1 -= lrate*d_E_b_FF1

"""# **4. Training Data**"""

x = 'what color is my dog'
y = 'my dog is brown'

trainX = S2Vec(x)
trainY = S2Vec(y)

w = np.shape(trainX)[1]

print(trainX)
print(trainY)

"""# **5. Test the Model**"""

init_params(dm, h, dk, dv, N, v, r)
O_E = encoder(trainX)
genOut = np.array([[0]])
Yhat = decoder(genOut, O_E)

Y = np.zeros((v, 1))
Y[trainY[0,1], 0] = 1
L = -1*np.dot(Y.T, np.log(Yhat))
outputWord = Vocab[np.argmax(Yhat),0]
wordIdx = np.argmax(Yhat)
print(L[0,0])

"""# **6. Train Model**"""

dataX = ['how often do humans eat',
         'what do humans eat',
         'what do dogs eat',
         'what do cars eat']
dataY = ['humans eat multiple meals per day',
         'humans eat fruits and meats but they do not eat other humans',
         'dogs eat dog food from their dog bowl and sometimes they get scraps',
         'cars eat gasoline']

# Initialize
init_params(dm, h, dk, dv, N, v, r)

# Run through training steps
step = 0
lrate = 0.005
steps = 500
while step < steps:
  loss = 0
  # Run through samples
  sample = 0
  numSamples = np.shape(dataX)[0]
  print(f"Training Step: {step}")
  while sample < numSamples:
    # Loop through testY
    trainX = S2Vec(dataX[sample])
    trainY = S2Vec(dataY[sample])
    senLength = np.shape(trainY)[1]
    trainIdx = 0
    senOut = ['<start>']
    while trainIdx < senLength-1:
      # Starts genOut from idx 0 (the start token) with Y starting at index 1
      O_E = encoder(trainX)
      genOut = np.reshape(trainY[0,0:trainIdx+1], (1,trainIdx+1))
      wordIdx = trainY[0,trainIdx+1]
      Y = one_hot_Y(wordIdx)
      Yhat = decoder(genOut, O_E)
      L = -1*np.dot(Y.T, np.log(Yhat))
      loss += L[0,0] / numSamples
      outputWordIdx = np.argmax(Yhat)
      outputWord = Vocab[outputWordIdx,0]
      d_D2_W_L, d_D2_W_FF2, d_D2_b_FF2, d_D2_W_FF1, d_D2_b_FF1, d_D2_W_O, d_D2_W_Q, d_D2_W_K, d_D2_W_V, d_I_db, d_D1_W_O, d_D1_W_V, d_D1_W_Q, d_D1_W_K, d_E_W_FF2, d_E_b_FF2, d_E_W_FF1, d_E_b_FF1, d_E_W_O, d_E_W_V, d_E_W_Q, d_E_W_K = get_gradients()
      update_params()
      senOut.append(outputWord)
      trainIdx += 1
    print(senOut)
    sample += 1
  print(f"loss = {loss}")
  print(f"lrate = {lrate}\n")
  lrate *= 0.9
  step += 1